# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=neo4j
NEO4J_QUERY_TIMEOUT=10.0
NEO4J_CONNECTION_TIMEOUT=5.0

# Pipeline Configuration
CODEBASE_PATH=/path/to/ansible/codebase
BATCH_SIZE=100
MAX_WORKERS=4
LOG_LEVEL=INFO

# Git Configuration
GIT_REPO_URL=
GIT_BRANCH=main
GIT_TOKEN=
WORKSPACE_DIR=/workspace

# LLM Configuration
LLM_PROVIDER=vllm
API_BASE=http://localhost:11434/v1
MODEL_NAME=Qwen/Qwen2.5-Coder-7B-Instruct
TEMPERATURE=0.0
MAX_TOKENS=2048
TOP_P=0.95

# MCP Server
MCP_SERVER_HOST=127.0.0.1
MCP_SERVER_PORT=5003
MCP_REQUIRE_AUTH=false
MCP_DEBUG=false

# Rate Limiting
MCP_RATE_LIMIT_PER_MINUTE=100
MCP_RATE_LIMIT_BURST=10

# AI-Dev Workflow Configuration (for Gemini CLI)
# Project identification for MCP servers
PLANE_WORKSPACE_SLUG=ai-dev
PLANE_PROJECT_SLUG=your-project-uuid
GITEA_OWNER=your-gitea-username
GITEA_DEFAULT_REPO=graphrag-pipeline
OUTLINE_SPECS_COLLECTION_ID=your-specs-collection-uuid
OUTLINE_REVIEWS_COLLECTION_ID=your-reviews-collection-uuid

# MCP Server URLs (for gemini-cli VM via Tailscale)
# Replace 'helios' with your host machine's Tailscale hostname
GEMINI_PLANE_API_URL=http://helios:4004/api/v1
GEMINI_OUTLINE_API_URL=http://helios:4003/api
GEMINI_GITEA_API_URL=http://helios:4001/api/v1

# Langfuse Configuration
LANGFUSE_ENABLED=false
LANGFUSE_PUBLIC_KEY=pk-lf-...
LANGFUSE_SECRET_KEY=sk-lf-...
LANGFUSE_HOST=https://cloud.langfuse.com

# LlamaStack Configuration
LLAMA_STACK_DIR=/path/to/llama-stack
VLLM_URL=http://localhost:11434/v1
VLLM_API_TOKEN=token-abc123
VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct
