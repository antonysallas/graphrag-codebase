[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "graphrag-pipeline"
version = "0.1.0"
description = "GraphRAG pipeline for Ansible codebase analysis using AST parsing and Neo4j"
readme = "README.md"
requires-python = ">=3.13"
authors = [
    {name = "GraphRAG Pipeline", email = "dev@example.com"}
]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13"
]

dependencies = [
    # Tree-sitter for AST parsing
    "tree-sitter>=0.21.0",
    # Individual tree-sitter language grammars (lighter than tree-sitter-languages)
    "tree-sitter-python==0.25.0",
    "tree-sitter-yaml==0.7.2",
    "tree-sitter-ruby==0.23.1",
    "tree-sitter-jinja==0.3.3",
    # Neo4j graph database
    "neo4j>=5.14.0",
    # Git operations
    "GitPython>=3.1.40",
    # YAML parsing
    "PyYAML>=6.0",
    "ruamel.yaml>=0.18.0",
    # Utilities
    "python-dotenv>=1.0.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "rich>=13.0.0", # Beautiful console output
    "typer>=0.9.0", # CLI framework
    # Logging
    "loguru>=0.7.0",
    # MCP Core
    "mcp>=0.9.0",
    # HTTP Server (for HTTP/SSE transport)
    "uvicorn>=0.27.0",
    "starlette>=0.36.0",
    "sse-starlette>=1.8.0",
    # LlamaIndex for GraphRAG
    "llama-index-core>=0.14.0",
    "llama-index-llms-openai-like>=0.5.0",
    "llama-index-graph-stores-neo4j>=0.5.0",
    # Telemetry (optional)
    "opentelemetry-api>=1.38.0",
    "opentelemetry-sdk>=1.38.0",
    "opentelemetry-exporter-jaeger>=1.21.0",
    "langfuse>=2.0.0",
    "gradio>=4.0.0",
    "llama-stack-client>=0.3.5",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.0",
    "pytest-asyncio>=0.21.0",
    "pytest-cov>=4.1.0",
    "black>=23.0.0",
    "mypy>=1.5.0",
    "ruff>=0.1.0",
    "types-PyYAML>=6.0.0",
]

docs = [
    "mkdocs>=1.6",
    "mkdocs-material>=9.5",
    "pymdown-extensions>=10.0",
]

[project.scripts]
graphrag-build = "scripts.build_graph:main"
graphrag-mcp = "src.mcp.server:main"
graphrag-mcp-http = "src.mcp.http_server:main"

[tool.setuptools.packages.find]
where = ["."]
include = ["src*", "scripts*"]

[tool.black]
line-length = 100
target-version = ['py39', 'py310', 'py311']
include = '\.pyi?$'

[tool.mypy]
python_version = "3.13"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
ignore_missing_imports = true

[tool.ruff]
line-length = 100
target-version = "py39"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "-v --cov=src --cov-report=html --cov-report=term"

[tool.uv.sources]
llama-stack-client = { git = "https://github.com/meta-llama/llama-stack-client-python.git" }
